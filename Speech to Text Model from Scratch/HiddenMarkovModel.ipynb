{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":492337,"sourceType":"datasetVersion","datasetId":230420},{"sourceId":2739456,"sourceType":"datasetVersion","datasetId":1670098}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch torchaudio hmmlearn numpy tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-31T12:46:27.981092Z","iopub.execute_input":"2025-03-31T12:46:27.981378Z","iopub.status.idle":"2025-03-31T12:46:32.509923Z","shell.execute_reply.started":"2025-03-31T12:46:27.981356Z","shell.execute_reply":"2025-03-31T12:46:32.509027Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nCollecting hmmlearn\n  Downloading hmmlearn-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.2.2)\nRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\nDownloading hmmlearn-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.6/164.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: hmmlearn\nSuccessfully installed hmmlearn-0.3.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torchaudio\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom hmmlearn import hmm\nimport numpy as np\nfrom tqdm import tqdm\nimport string\nimport csv\nfrom pathlib import Path\nfrom jiwer import wer, cer  # For WER and CER calculation\n\n# ======================\n# Dataset Configuration\n# ======================\nroot_path = '/kaggle/input/librispeech-clean'\ntrain_dataset = torchaudio.datasets.LIBRISPEECH(root_path, url=\"train-clean-100\", download=False)\ntest_dataset = torchaudio.datasets.LIBRISPEECH(root_path, url=\"test-clean\", download=False)\n\n# =====================\n# Audio Preprocessing\n# =====================\naudio_transforms = torchaudio.transforms.MelSpectrogram(\n    sample_rate=16000,\n    n_mels=80,        # Optimized for speech recognition\n    n_fft=512,        # Proper frequency resolution\n    hop_length=160,   # 10ms frame shift\n    win_length=400    # 25ms window size\n)\n\n# ====================\n# Text Transformation\n# ====================\nclass TextTransform:\n    def __init__(self):\n        self.char_map = {'': 0, ' ': 1, **{c: i+2 for i, c in enumerate(string.ascii_lowercase)}}\n        self.index_map = {v: k for k, v in self.char_map.items()}\n\n    def text_to_int(self, text):\n        return [self.char_map.get(c, 1) for c in text.lower()]\n\n    def int_to_text(self, labels):\n        return ''.join(self.index_map.get(i, ' ') for i in labels).strip()\n\ntext_transform = TextTransform()\n\n# ==================\n# Data Processing\n# ==================\ndef data_processing(data):\n    spectrograms, labels, input_lengths, label_lengths = [], [], [], []\n    for waveform, _, utterance, _, _, _ in data:\n        # Audio processing\n        spec = audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        spectrograms.append(spec)\n        \n        # Text processing\n        label = torch.tensor(text_transform.text_to_int(utterance), dtype=torch.long)\n        labels.append(label)\n        \n        # Length calculations\n        input_lengths.append(spec.shape[0])  # Full sequence length\n        label_lengths.append(len(label))\n    \n    # Batch padding\n    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1)\n    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n    \n    return spectrograms, labels, input_lengths, label_lengths\n\n# ================\n# Model Architecture\n# ================\nclass SpeechRecognitionModel(nn.Module):\n    def __init__(self, n_cnn_layers=2, rnn_dim=512, n_class=28, n_feats=80, dropout=0.1):\n        super().__init__()\n        \n        # CNN Layers\n        self.cnn = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            *[nn.Sequential(\n                nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n                nn.BatchNorm2d(32),\n                nn.ReLU(),\n                nn.Dropout(dropout)\n            ) for _ in range(n_cnn_layers-1)]\n        )\n        \n        # RNN Layers\n        self.rnn = nn.LSTM(\n            input_size=32*n_feats,  # 32 channels * 80 mel bands\n            hidden_size=rnn_dim,\n            num_layers=2,\n            bidirectional=True,\n            batch_first=True\n        )\n        \n        # Final Classifier\n        self.classifier = nn.Linear(rnn_dim*2, n_class)\n\n    def forward(self, x):\n        # CNN processing\n        x = self.cnn(x)  # (batch, 32, time, n_feats)\n        \n        # Dimension reshaping\n        x = x.permute(0, 2, 1, 3)  # (batch, time, 32, n_feats)\n        x = x.flatten(2)            # (batch, time, 32*n_feats)\n        \n        # RNN processing\n        x, _ = self.rnn(x)  # (batch, time, rnn_dim*2)\n        \n        # Classification\n        return self.classifier(x)\n\n# ================\n# Training Setup\n# ================\nparams = {\n    'batch_size': 10,\n    'epochs': 10,\n    'learning_rate': 5e-4,\n    'n_class': 28,  # 26 letters + space + blank\n    'n_feats': 80   # Must match MelSpectrogram n_mels\n}\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize model\nmodel = SpeechRecognitionModel().to(device)\noptimizer = optim.AdamW(model.parameters(), lr=params['learning_rate'])\ncriterion = nn.CTCLoss(blank=0)\n\n# Data loaders\ntrain_loader = DataLoader(train_dataset, \n                         batch_size=params['batch_size'],\n                         shuffle=True,\n                         collate_fn=data_processing,\n                         num_workers=4,\n                         pin_memory=True)\n\ntest_loader = DataLoader(test_dataset,\n                        batch_size=params['batch_size'],\n                        shuffle=False,\n                        collate_fn=data_processing,\n                        num_workers=4,\n                        pin_memory=True)\n\n# ================\n# Training Loop\n# ================\nfor epoch in range(params['epochs']):\n    model.train()\n    epoch_loss = 0\n    \n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{params['epochs']}\"):\n        spectrograms, labels, input_lengths, label_lengths = batch\n        \n        # Move data to device\n        spectrograms = spectrograms.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        optimizer.zero_grad()\n        outputs = model(spectrograms)\n        outputs = F.log_softmax(outputs, dim=2)\n        \n        # CTC Loss requirements: (T, N, C)\n        outputs = outputs.permute(1, 0, 2)\n        \n        loss = criterion(\n            outputs,\n            labels,\n            input_lengths=torch.tensor(input_lengths),\n            target_lengths=torch.tensor(label_lengths)\n        )\n        \n        # Backward pass\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1} | Avg Loss: {epoch_loss/len(train_loader):.4f}\")\n\n# Save trained model\ntorch.save(model.state_dict(), '/kaggle/working/speech_model.pth')\n\n# ================\n# HMM Integration\n# ================\ndef prepare_hmm_data():\n    model.eval()\n    all_sequences = []\n    lengths = []\n    \n    with torch.no_grad():\n        for batch in tqdm(train_loader, desc=\"Preparing HMM Data\"):\n            spectrograms, _, input_lengths, _ = batch\n            outputs = model(spectrograms.to(device))\n            emissions = torch.argmax(outputs, dim=2).cpu().numpy()\n            \n            for i, length in enumerate(input_lengths):\n                all_sequences.append(emissions[i, :length])\n                lengths.append(length)\n    \n    return np.concatenate(all_sequences), np.array(lengths)\n\n# Train HMM\nhmm_observations, hmm_lengths = prepare_hmm_data()\nhmm_model = hmm.GaussianHMM(\n    n_components=params['n_class'],\n    covariance_type=\"diag\",\n    n_iter=100\n)\nhmm_model.fit(hmm_observations.reshape(-1, 1), lengths=hmm_lengths)\n\n# ====================\n# Inference Function\n# ====================\ndef speech_to_text(waveform):\n    model.eval()\n    with torch.no_grad():\n        # Process audio\n        spec = audio_transforms(waveform).unsqueeze(0).to(device)\n        \n        # Model predictions\n        outputs = model(spec)\n        emissions = torch.argmax(outputs, dim=2).cpu().numpy()[0]\n        \n        # HMM decoding\n        _, best_path = hmm_model.decode(emissions.reshape(-1, 1))\n        \n    return text_transform.int_to_text(best_path)\n\n# ====================\n# Evaluation & Output with WER and CER\n# ====================\ndef generate_results():\n    results = []\n    model.eval()\n    ground_truths = []\n    predictions = []\n    \n    with torch.no_grad():\n        for i in tqdm(range(len(test_dataset)), desc=\"Evaluating\"):\n            waveform, _, utterance, sid, cid, uid = test_dataset[i]\n            prediction = speech_to_text(waveform)\n            \n            # Store for WER/CER calculation\n            ground_truths.append(utterance.lower())\n            predictions.append(prediction)\n            \n            # Store results for CSV\n            results.append([\n                f\"{sid}-{cid}-{uid}\",\n                utterance.lower(),\n                prediction\n            ])\n    \n    # Calculate WER and CER\n    wer_score = wer(ground_truths, predictions)\n    cer_score = cer(ground_truths, predictions)\n    \n    # Print WER and CER\n    print(f\"Word Error Rate (WER): {wer_score:.4f}\")\n    print(f\"Character Error Rate (CER): {cer_score:.4f}\")\n    \n    # Save results to CSV\n    with open('/kaggle/working/stt_results.csv', 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow([\"ID\", \"Ground Truth\", \"Prediction\"])\n        writer.writerows(results)\n    \n    return wer_score, cer_score\n\n# Run evaluation\nwer_score, cer_score = generate_results()\nprint(\"Processing complete! Results saved to stt_results.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:15:28.305944Z","iopub.execute_input":"2025-04-10T16:15:28.306259Z","iopub.status.idle":"2025-04-10T16:15:28.310991Z","shell.execute_reply.started":"2025-04-10T16:15:28.306238Z","shell.execute_reply":"2025-04-10T16:15:28.310050Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/10: 100%|██████████| 2854/2854 [28:36<00:00,  1.66it/s]\nEpoch 1 | Avg Loss: 1.6857\nEpoch 2/10: 100%|██████████| 2854/2854 [28:34<00:00,  1.66it/s]\nEpoch 2 | Avg Loss: 0.7422\nEpoch 3/10: 100%|██████████| 2854/2854 [28:36<00:00,  1.66it/s]\nEpoch 3 | Avg Loss: 0.5444\nEpoch 4/10: 100%|██████████| 2854/2854 [28:37<00:00,  1.66it/s]\nEpoch 4 | Avg Loss: 0.4360\nEpoch 5/10: 100%|██████████| 2854/2854 [28:35<00:00,  1.66it/s]\nEpoch 5 | Avg Loss: 0.3652\nEpoch 6/10: 100%|██████████| 2854/2854 [28:35<00:00,  1.66it/s]\nEpoch 6 | Avg Loss: 0.3112\nEpoch 7/10: 100%|██████████| 2854/2854 [28:32<00:00,  1.67it/s]\nEpoch 7 | Avg Loss: 0.2699\nEpoch 8/10: 100%|██████████| 2854/2854 [28:31<00:00,  1.67it/s]\nEpoch 8 | Avg Loss: 0.2359\nEpoch 9/10: 100%|██████████| 2854/2854 [28:31<00:00,  1.67it/s]\nEpoch 9 | Avg Loss: 0.2100\nEpoch 10/10: 100%|██████████| 2854/2854 [28:31<00:00,  1.67it/s]\nEpoch 10 | Avg Loss: 0.1861\nPreparing HMM Data: 100%|██████████| 2854/2854 [09:37<00:00,  4.94it/s]\nEvaluating\nWord Error Rate (WER): 0.2762\nCharacter Error Rate (CER): 0.1269\nProcessing complete! Results saved to stt_results.csv\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}